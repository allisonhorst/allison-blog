---
title: "Exploring tree outcomes following fires"
author: Allison Horst
date: '2022-03-10'
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="overview" class="section level2">
<h2>Overview</h2>
<p>Basically, there’s this awesome dataset on tree survival following fires, the <a href="https://doi.org/10.2737/RDS-2020-0001">Fire and Tree Mortality Database</a>, and I want to go exploring &amp; compare fire survival across species. Some fun with <code>tidymodels</code>, data visualization, binary logistic regression, and my first shot at using the fantastic <code>geomtextpath</code> package!</p>
</div>
<div id="citations" class="section level2">
<h2>Citations:</h2>
<p>Cansler, C. Alina; Hood, Sharon M.; Varner, J. Morgan; van Mantgem, Phillip J.; Agne, Michelle C.; Andrus, Robert A.; Ayres, Matthew P.; Ayres, Bruce D.; Bakker, Jonathan D.; Battaglia, Michael A.; Bentz, Barbara J.; Breece, Carolyn R.; Brown, James K.; Cluck, Daniel R.; Coleman, Tom W.; Corace, R. Gregory; Covington, W. Wallace; Cram, Douglas S.; Cronan, James B.; Crouse, Joseph E.; Das, Adrian J.; Davis, Ryan S.; Dickinson, Darci M.; Fitzgerald, Stephen A.; Fulé, Peter Z.; Ganio, Lisa M.; Grayson, Lindsay M.; Halpern, Charles B.; Hanula, Jim L.; Harvey, Brian J.; Hiers, J. Kevin; Huffman, David W.; Keifer, MaryBeth; Keyser, Tara L.; Kobziar, Leda N.; Kolb, Thomas E.; Kolden, Crystal A.; Kopper, Karen E.; Kreitler, Jason R.; Kreye, Jesse K.; Latimer, Andrew M.; Lerch, Andrew P.; Lombardero, Maria J.; McDaniel, Virginia L.; McHugh, Charles W.; McMillin, Joel D.; Moghaddas, Jason J.; O’Brien, Joseph J.; Perrakis, Daniel D.B.; Peterson, David W.; Prichard, Susan J.; Progar, Robert A.; Raffa, Kenneth F.; Reinhardt, Elizabeth D.; Restaino, Joseph C.; Roccaforte, John P.; Rogers, Brendan M.; Ryan, Kevin C.; Safford, Hugh D.; Santoro, Alyson E.; Shearman, Timothy M.; Shumate, Alice M.; Sieg, Carolyn H.; Smith, Sheri L.; Smith, Rebecca J.; Stephenson, Nathan L.; Steuver, Mary; Stevens, Jens T.; Stoddard, Michael T.; Thies, Walter G.; Vaillant, Nicole M.; Weiss, Shelby A.; Westlind, Douglas J.; Woolley, Travis J.; Wright, Michah. 2020. Fire and tree mortality database (FTM). Fort Collins, CO: Forest Service Research Data Archive. Updated 24 July 2020. <a href="https://doi.org/10.2737/RDS-2020-0001" class="uri">https://doi.org/10.2737/RDS-2020-0001</a></p>
<p>Cansler, C. Alina; Hood, Sharon M.; Varner, J. Morgan; van Mantgem, Phillip J.; Agne, Michelle C.; Andrus, Robert A.; Ayres, Matthew P.; Ayres, Bruce D.; Bakker, Jonathan D.; Battaglia, Michael A.; Bentz, Barbara J.; Breece, Carolyn R.; Brown, James K.; Cluck, Daniel R.; Coleman, Tom W.; Corace, R. Gregory; Covington, W. Wallace; Cram, Douglas S.; Cronan, James B.; Crouse, Joseph E.; Das, Adrian J.; Davis, Ryan S.; Dickinson, Darci M.; Fitzgerald, Stephen A.; Fulé, Peter Z.; Ganio, Lisa M.; Grayson, Lindsay M.; Halpern, Charles B.; Hanula, Jim L.; Harvey, Brian J.; Hiers, J. Kevin; Huffman, David W.; Keifer, MaryBeth; Keyser, Tara L.; Kobziar, Leda N.; Kolb, Thomas E.; Kolden, Crystal A.; Kopper, Karen E.; Kreitler, Jason R.; Kreye, Jesse K.; Latimer, Andrew M.; Lerch, Andrew P.; Lombardero, Maria J.; McDaniel, Virginia L.; McHugh, Charles W.; McMillin, Joel D.; Moghaddas, Jason J.; O’Brien, Joseph J.; Perrakis, Daniel D.B.; Peterson, David W.; Prichard, Susan J.; Progar, Robert A.; Raffa, Kenneth F.; Reinhardt, Elizabeth D.; Restaino, Joseph C.; Roccaforte, John P.; Rogers, Brendan M.; Ryan, Kevin C.; Safford, Hugh D.; Santoro, Alyson E.; Shearman, Timothy M.; Shumate, Alice M.; Sieg, Carolyn H.; Smith, Sheri L.; Smith, Rebecca J.; Stephenson, Nathan L.; Steuver, Mary; Stevens, Jens T.; Stoddard, Michael T.; Thies, Walter G.; Vaillant, Nicole M.; Weiss, Shelby A.; Westlind, Douglas J.; Woolley, Travis J.; Wright, Michah C. 2020. The Fire and Tree Mortality Database, for empirical modeling of individual tree mortality after fire. Scientific Data 7: 194. <a href="https://doi.org/10.1038/s41597-020-0522-7" class="uri">https://doi.org/10.1038/s41597-020-0522-7</a></p>
</div>
<div id="attach-packages-read-in-the-data" class="section level2">
<h2>Attach packages &amp; read in the data</h2>
<pre class="r"><code>library(tidyverse)
library(here)
library(naniar)
library(tidymodels)
library(geomtextpath)
library(paletteer)</code></pre>
<pre class="r"><code>fires &lt;- read_csv(here(&quot;content&quot;, &quot;post&quot;, &quot;2022-03-10-tree-mortality-fires&quot;, &quot;data&quot;, &quot;Data&quot;, &quot;FTM_fires.csv&quot;)) # Fire information
trees &lt;- read_csv(here(&quot;content&quot;, &quot;post&quot;, &quot;2022-03-10-tree-mortality-fires&quot;, &quot;data&quot;, &quot;Data&quot;, &quot;FTM_trees.csv&quot;)) # Tree outcomes and records
bark &lt;- read_csv(here(&quot;content&quot;, &quot;post&quot;, &quot;2022-03-10-tree-mortality-fires&quot;, &quot;data&quot;, &quot;Data&quot;, &quot;Species_BarkThickness.csv&quot;)) # Bark thickness </code></pre>
<p>Important information: See attributes in <code>_metadata_RDS-2020-0001.html</code> for variable definitions.</p>
</div>
<div id="exploratory-data-visualization" class="section level2">
<h2>Exploratory data visualization</h2>
<p>Counts of tree species in the dataset:</p>
<pre class="r"><code># Find the top 20 most counted tree species
trees &lt;- trees %&gt;% 
  mutate(sci_name = paste(Genus, Species_name)) %&gt;% 
  filter(sci_name != &quot;Pinus jeffreyi or ponderosa&quot;)

tree_count_top_20 &lt;- trees %&gt;% 
  count(sci_name) %&gt;% 
  mutate(sci_name = fct_reorder(sci_name, n)) %&gt;% 
  slice_max(n, n = 20)

ggplot(data = tree_count_top_20, aes(x = sci_name, y = n)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(y = &quot;\nObservations in dataset&quot;,
       x = &quot;Scientific name&quot;)</code></pre>
<p><img src="/post/2022-03-10-tree-mortality-fires/fire_tree_mortality_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Counts of live (0) and dead (1) for the top 20 most recorded trees in the dataset:</p>
<pre class="r"><code># Make a long form of the trees dataset (top 20 most observed tree species)
trees_long &lt;- trees %&gt;% 
  pivot_longer(cols = yr0status:yr10status, names_to = &quot;yr_outcome&quot;, values_to = &quot;live_dead&quot;) %&gt;% 
  mutate(yr_since_fire = as.numeric(parse_number(yr_outcome)),
         live_dead_chr = case_when(
           live_dead == 0 ~ &quot;live&quot;,
           live_dead == 1 ~ &quot;dead&quot;
         )) %&gt;% 
  filter(sci_name %in% tree_count_top_20$sci_name)

trees_live_dead &lt;- trees_long %&gt;% 
  count(sci_name, yr_since_fire, live_dead_chr) %&gt;% 
  drop_na()
  
ggplot(data = trees_live_dead, aes(x = yr_since_fire, y = n)) +
  geom_col(aes(fill = live_dead_chr), position = &quot;fill&quot;) +
  scale_fill_manual(values = c(&quot;lightsalmon&quot;, &quot;forestgreen&quot;),
                    name = &quot;Live / dead:&quot;) +
  scale_x_continuous(breaks = c(0, 5, 10), labels = c(&quot;0&quot;, &quot;5&quot;, &quot;10&quot;)) +
  theme_minimal() +
  labs(x = &quot;Years since fire&quot;,
       y = &quot;Proportion live / dead&quot;,
       title = &quot;Tree survival post-fire&quot;,
       subtitle = &quot;Only includes the 20 most observed trees in the dataset&quot;,
       caption = &quot;Data: Fire and tree mortality database (FTM)&quot;) +
  facet_wrap(~sci_name)</code></pre>
<p><img src="/post/2022-03-10-tree-mortality-fires/fire_tree_mortality_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
<p>We can already see some interesting differences in survival across species. For example, <em>Picea mariana</em> and <em>Abies lasiocarpa</em> experience quick mortality within the first year; others like <em>Pinus jeffreyi</em> and <em>Abies concolor</em> appear more resilient. However, near-complete mortality is observed across all species within 10 years.</p>
</div>
<div id="ponderosa-pines---diving-a-bit-deeper" class="section level2">
<h2>Ponderosa pines - diving a bit deeper</h2>
<p>Since it is the most observed species in the dataset <em>and</em> because it happens to be one of my favorite trees, I’ll dive a bit deeper into factors that may influence <em>Pinus ponderosa</em> mortality post-fire.</p>
<pre class="r"><code>ponderosa &lt;- trees_long %&gt;% 
  filter(sci_name == &quot;Pinus ponderosa&quot;)</code></pre>
<p>First, let’s take a look at mortality over time (years since fire):</p>
<pre class="r"><code>ggplot(data = ponderosa, aes(x = yr_since_fire, y = live_dead)) +
  geom_jitter(alpha = 0.008) +
  labs(x = &quot;Years since fire&quot;,
       y = &quot;Tree status (live / dead)&quot;,
       title = &quot;Ponderosa pine mortality post-fire&quot;,
       caption = &quot;Data: Fire and tree mortality database (FTM)&quot;) +
  scale_y_continuous(breaks = c(0, 1), labels = c(&quot;Live&quot;, &quot;Dead&quot;)) +
  scale_x_continuous(breaks = c(0, 5, 10), labels = c(&quot;0&quot;, &quot;5&quot;, &quot;10&quot;)) +
  theme_minimal()</code></pre>
<p><img src="/post/2022-03-10-tree-mortality-fires/fire_tree_mortality_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
<div id="classification-binary-logistic-regression-in-tidymodels" class="section level3">
<h3>Classification: binary logistic regression in <code>tidymodels</code></h3>
<p>Create the training &amp; testing sets</p>
<pre class="r"><code>ponderosa &lt;- ponderosa %&gt;% 
  drop_na(yr_since_fire, live_dead) %&gt;% 
  mutate(live_dead = as.factor(live_dead))

# Make the training &amp; testing dataset:
ponderosa_split &lt;- ponderosa %&gt;% 
  initial_split(prop = 4/5)

# Confirm the splits (Analysis/Assess/Total): 
ponderosa_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;293297/73325/366622&gt;</code></pre>
<pre class="r"><code># Extract the training and testing sets: 
ponderosa_train &lt;- training(ponderosa_split)
ponderosa_test &lt;- testing(ponderosa_split)

# Check them out a bit: 
ponderosa_train %&gt;% 
  count(yr_since_fire, live_dead)</code></pre>
<pre><code>## # A tibble: 22 × 3
##    yr_since_fire live_dead     n
##            &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;
##  1             0 0         39824
##  2             0 1           561
##  3             1 0         39072
##  4             1 1         10385
##  5             2 0         26862
##  6             2 1         12835
##  7             3 0         23708
##  8             3 1         14808
##  9             4 0          8620
## 10             4 1         15011
## # … with 12 more rows</code></pre>
<pre class="r"><code>ponderosa_test %&gt;% 
  count(yr_since_fire, live_dead)</code></pre>
<pre><code>## # A tibble: 22 × 3
##    yr_since_fire live_dead     n
##            &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;
##  1             0 0          9845
##  2             0 1           128
##  3             1 0          9807
##  4             1 1          2584
##  5             2 0          6846
##  6             2 1          3259
##  7             3 0          5845
##  8             3 1          3693
##  9             4 0          2183
## 10             4 1          3742
## # … with 12 more rows</code></pre>
</div>
<div id="make-a-recipe" class="section level3">
<h3>Make a recipe</h3>
<pre class="r"><code># Just using the single predictor here:
ponderosa_recipe &lt;- recipe(live_dead ~ yr_since_fire, data = ponderosa)
ponderosa_recipe </code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          1</code></pre>
</div>
<div id="make-the-model" class="section level3">
<h3>Make the model</h3>
<pre class="r"><code>ponderosa_model &lt;- 
  logistic_reg() %&gt;%
  set_engine(&quot;glm&quot;) %&gt;%
  set_mode(&quot;classification&quot;) # Binary classificiation</code></pre>
</div>
<div id="make-the-workflow" class="section level3">
<h3>Make the workflow</h3>
<pre class="r"><code>ponderosa_wf &lt;- workflow() %&gt;% 
  add_recipe(ponderosa_recipe) %&gt;% 
  add_model(ponderosa_model)</code></pre>
</div>
<div id="fit-the-model" class="section level3">
<h3>Fit the model:</h3>
<pre class="r"><code>ponderosa_fit &lt;- ponderosa_wf %&gt;% 
  last_fit(ponderosa_split)

# Which returns high accuracy and roc_auc:
ponderosa_fit %&gt;% collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 × 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.805 Preprocessor1_Model1
## 2 roc_auc  binary         0.878 Preprocessor1_Model1</code></pre>
</div>
<div id="proof-of-concept-check-out-the-test-set-predictions" class="section level3">
<h3>Proof of concept: check out the test set predictions</h3>
<p>…just for the first 20 rows:</p>
<pre class="r"><code>ponderosa_fit %&gt;% 
  collect_predictions() %&gt;% 
  head(20)</code></pre>
<pre><code>## # A tibble: 20 × 7
##    id               .pred_0 .pred_1  .row .pred_class live_dead .config         
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;     &lt;chr&gt;           
##  1 train/test split  0.722   0.278      2 0           1         Preprocessor1_M…
##  2 train/test split  0.566   0.434     14 0           0         Preprocessor1_M…
##  3 train/test split  0.912   0.0877    25 0           0         Preprocessor1_M…
##  4 train/test split  0.839   0.161     26 0           0         Preprocessor1_M…
##  5 train/test split  0.839   0.161     29 0           1         Preprocessor1_M…
##  6 train/test split  0.0391  0.961     36 1           1         Preprocessor1_M…
##  7 train/test split  0.0200  0.980     37 1           1         Preprocessor1_M…
##  8 train/test split  0.839   0.161     44 0           0         Preprocessor1_M…
##  9 train/test split  0.839   0.161     51 0           1         Preprocessor1_M…
## 10 train/test split  0.246   0.754     55 1           1         Preprocessor1_M…
## 11 train/test split  0.140   0.860     56 1           1         Preprocessor1_M…
## 12 train/test split  0.0391  0.961     58 1           1         Preprocessor1_M…
## 13 train/test split  0.839   0.161     66 0           0         Preprocessor1_M…
## 14 train/test split  0.0101  0.990     75 1           1         Preprocessor1_M…
## 15 train/test split  0.140   0.860     89 1           1         Preprocessor1_M…
## 16 train/test split  0.246   0.754    108 1           1         Preprocessor1_M…
## 17 train/test split  0.0200  0.980    112 1           1         Preprocessor1_M…
## 18 train/test split  0.839   0.161    115 0           0         Preprocessor1_M…
## 19 train/test split  0.912   0.0877   118 0           0         Preprocessor1_M…
## 20 train/test split  0.839   0.161    119 0           0         Preprocessor1_M…</code></pre>
</div>
<div id="confusion-matrix-of-truth-predictions" class="section level3">
<h3>Confusion matrix of truth / predictions</h3>
<p>Recall here: 0 = “Live”, 1 = “Dead”</p>
<pre class="r"><code>ponderosa_fit %&gt;% 
  collect_predictions() %&gt;% 
  conf_mat(truth = live_dead, estimate = .pred_class)</code></pre>
<pre><code>##           Truth
## Prediction     0     1
##          0 32343  9664
##          1  4605 26713</code></pre>
</div>
<div id="fit-on-entire-dataset" class="section level3">
<h3>Fit on entire dataset</h3>
<pre class="r"><code>ponderosa_model_full &lt;- fit(ponderosa_wf, ponderosa)

ponderosa_model_full</code></pre>
<pre><code>## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: logistic_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 0 Recipe Steps
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## 
## Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)
## 
## Coefficients:
##   (Intercept)  yr_since_fire  
##       -2.3424         0.6929  
## 
## Degrees of Freedom: 366621 Total (i.e. Null);  366620 Residual
## Null Deviance:       508200 
## Residual Deviance: 318000    AIC: 318000</code></pre>
</div>
<div id="making-new-predictions" class="section level3">
<h3>Making new predictions</h3>
<p>Let’s say we want to predict survival of other ponderosa pines based solely on years post-fire:</p>
<pre class="r"><code># Make a data frame containing a &quot;yr_since_fire&quot; variable as a new model input:
new_yr &lt;- data.frame(yr_since_fire = c(0, 0.4, 1, 2.2, 5.7, 8.3))

# Then use the model to predict outcomes, bind together: 
example_predictions &lt;- data.frame(new_yr, predict(ponderosa_model_full, new_yr))

example_predictions</code></pre>
<pre><code>##   yr_since_fire .pred_class
## 1           0.0           0
## 2           0.4           0
## 3           1.0           0
## 4           2.2           0
## 5           5.7           1
## 6           8.3           1</code></pre>
<p>This does seem to align with what we’d expect based on the data visualization. We can also find the <em>probability</em> of “Dead” (outcome = 1) using the model predictions, adding <code>type = "prob"</code> within the <code>predict()</code> function.</p>
<pre class="r"><code>predict_over &lt;- data.frame(yr_since_fire = seq(from = 0, to = 10, by = 0.1))
predictions_full &lt;- data.frame(predict_over, predict(ponderosa_model_full, predict_over, type = &quot;prob&quot;))
names(predictions_full) &lt;- c(&quot;yr_since_fire&quot;, &quot;prob_alive&quot;, &quot;prob_dead&quot;)

# Plot probability of mortality:
ggplot() +
  geom_line(data = predictions_full, aes(x = yr_since_fire, y = prob_alive), color = &quot;gray30&quot;, size = 1) +
  labs(x = &quot;Years since fire&quot;,
       y = &quot;Probability of tree being alive&quot;,
       title = &quot;Predicted Ponderosa pine mortality post-fire&quot;,
       caption = &quot;Data: Fire and tree mortality database (FTM)&quot;) +
  scale_y_continuous(breaks = c(0, 0.5,  1), 
                     labels = c(&quot;0%&quot;, &quot;50%&quot;, &quot;100%&quot;),
                     limits = c(0, 1)) +
  scale_x_continuous(breaks = c(0, 5, 10), labels = c(&quot;0&quot;, &quot;5&quot;, &quot;10&quot;)) +
  theme_minimal()</code></pre>
<p><img src="/post/2022-03-10-tree-mortality-fires/fire_tree_mortality_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
</div>
<div id="extending-the-model" class="section level2">
<h2>Extending the model</h2>
<p>I want to extend this model for the 20 most observed trees in the dataset (so will include species as a predictor variable).</p>
<p>Create the training &amp; testing sets</p>
<pre class="r"><code>trees_20 &lt;- trees_long %&gt;% 
  filter(sci_name %in% c(tree_count_top_20$sci_name)) %&gt;% 
  drop_na(yr_since_fire, live_dead) %&gt;% 
  mutate(live_dead = as.factor(live_dead))

# Make the training &amp; testing dataset:
trees_20_split &lt;- trees_20 %&gt;% 
  initial_split(prop = 4/5)

# Confirm the splits (Analysis/Assess/Total): 
trees_20_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;830790/207698/1038488&gt;</code></pre>
<pre class="r"><code># Extract the training and testing sets: 
trees_20_train &lt;- training(trees_20_split)
trees_20_test &lt;- testing(trees_20_split)

# Check them out a bit: 
trees_20_train %&gt;% 
  count(yr_since_fire, live_dead)</code></pre>
<pre><code>## # A tibble: 22 × 3
##    yr_since_fire live_dead     n
##            &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;
##  1             0 0         77760
##  2             0 1          2962
##  3             1 0         73452
##  4             1 1         30750
##  5             2 0         51549
##  6             2 1         47307
##  7             3 0         43185
##  8             3 1         54802
##  9             4 0         22368
## 10             4 1         55530
## # … with 12 more rows</code></pre>
<pre class="r"><code>trees_20_test %&gt;% 
  count(yr_since_fire, live_dead)</code></pre>
<pre><code>## # A tibble: 22 × 3
##    yr_since_fire live_dead     n
##            &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;
##  1             0 0         19366
##  2             0 1           692
##  3             1 0         18311
##  4             1 1          7444
##  5             2 0         12987
##  6             2 1         11759
##  7             3 0         10776
##  8             3 1         13666
##  9             4 0          5658
## 10             4 1         13913
## # … with 12 more rows</code></pre>
<div id="make-a-recipe-1" class="section level3">
<h3>Make a recipe</h3>
<pre class="r"><code># Just using the single predictor here:
trees_20_recipe &lt;- recipe(live_dead ~ yr_since_fire + sci_name, data = trees_20)
trees_20_recipe</code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          2</code></pre>
</div>
<div id="make-the-model-1" class="section level3">
<h3>Make the model</h3>
<pre class="r"><code>trees_20_model &lt;- 
  logistic_reg() %&gt;%
  set_engine(&quot;glm&quot;) %&gt;%
  set_mode(&quot;classification&quot;) # Binary classificiation</code></pre>
</div>
<div id="make-the-workflow-1" class="section level3">
<h3>Make the workflow</h3>
<pre class="r"><code>trees_20_wf &lt;- workflow() %&gt;% 
  add_recipe(trees_20_recipe) %&gt;% 
  add_model(trees_20_model)</code></pre>
</div>
<div id="fit-the-model-1" class="section level3">
<h3>Fit the model:</h3>
<pre class="r"><code>trees_20_fit &lt;- trees_20_wf %&gt;% 
  last_fit(trees_20_split)

# Which returns high accuracy and roc_auc:
trees_20_fit %&gt;% collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 × 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.822 Preprocessor1_Model1
## 2 roc_auc  binary         0.894 Preprocessor1_Model1</code></pre>
</div>
<div id="confusion-matrix-of-truth-predictions-1" class="section level3">
<h3>Confusion matrix of truth / predictions</h3>
<p>Recall here: 0 = “Live”, 1 = “Dead”</p>
<pre class="r"><code>trees_20_fit %&gt;% 
  collect_predictions() %&gt;% 
  conf_mat(truth = live_dead, estimate = .pred_class)</code></pre>
<pre><code>##           Truth
## Prediction      0      1
##          0  57706  19732
##          1  17250 113010</code></pre>
</div>
<div id="fit-on-entire-dataset-1" class="section level3">
<h3>Fit on entire dataset</h3>
<pre class="r"><code>trees_20_model_full &lt;- fit(trees_20_wf, trees_20)

trees_20_model_full</code></pre>
<pre><code>## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: logistic_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 0 Recipe Steps
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## 
## Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)
## 
## Coefficients:
##                   (Intercept)                  yr_since_fire  
##                      -1.89320                        0.61524  
##         sci_nameAbies grandis       sci_nameAbies lasiocarpa  
##                       0.69818                        2.72269  
##           sci_nameAcer rubrum   sci_nameCalocedrus decurrens  
##                       0.54265                        0.24419  
##  sci_nameJuniperus scopulorum     sci_nameLarix occidentalis  
##                       0.94475                       -1.21916  
##     sci_namePicea engelmannii          sci_namePicea mariana  
##                       1.88703                        5.68710  
##      sci_namePinus albicaulis         sci_namePinus contorta  
##                       2.17161                        1.78648  
##        sci_namePinus echinata         sci_namePinus jeffreyi  
##                       0.56136                       -0.54492  
##     sci_namePinus lambertiana        sci_namePinus palustris  
##                       0.30286                       -1.42278  
##       sci_namePinus ponderosa            sci_namePinus taeda  
##                      -0.21649                        0.06294  
##   sci_namePopulus tremuloides  sci_namePseudotsuga menziesii  
##                       1.30545                       -0.27530  
##    sci_nameTsuga heterophylla  
##                       0.82540  
## 
## Degrees of Freedom: 1038487 Total (i.e. Null);  1038467 Residual
## Null Deviance:       1358000 
## Residual Deviance: 818700    AIC: 818700</code></pre>
</div>
<div id="mortality-probability" class="section level3">
<h3>Mortality (probability)</h3>
<pre class="r"><code># Make a data frame containing a &quot;sci_name&quot; and &quot;yr_since_fire&quot; variable as a new model input:
new_data &lt;- data.frame(sci_name = rep(unique(trees_20$sci_name), 100)) %&gt;% 
  arrange(sci_name)

new_data &lt;- data.frame(new_data, yr_since_fire = rep(seq(from = 0, to = 10, length = 100), 20))

tree_20_predictions &lt;- data.frame(new_data, predict(trees_20_model_full, new_data, type = &quot;prob&quot;))
names(tree_20_predictions) &lt;- c(&quot;sci_name&quot;, &quot;yr_since_fire&quot;, &quot;prob_alive&quot;, &quot;prob_dead&quot;)

# Plot probability of mortality:
ggplot() +
  geom_textline(data = tree_20_predictions, 
                aes(x = yr_since_fire, 
                    y = prob_alive, 
                    label = sci_name,
                    group = sci_name,
                    color = sci_name), 
                size = 2.5,
                show.legend = FALSE) +
  labs(x = &quot;Years since fire&quot;,
       y = &quot;Probability of tree being alive&quot;,
       title = &quot;Predicted tree mortality post-fire&quot;,
       caption = &quot;Data: Fire and tree mortality database (FTM)&quot;) +
  scale_y_continuous(breaks = c(0, 0.5,  1), 
                     labels = c(&quot;0%&quot;, &quot;50%&quot;, &quot;100%&quot;),
                     limits = c(0, 1)) +
  scale_x_continuous(breaks = c(0, 5, 10), labels = c(&quot;0&quot;, &quot;5&quot;, &quot;10&quot;)) +
  scale_color_paletteer_d(&quot;ggthemes::Tableau_20&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/2022-03-10-tree-mortality-fires/fire_tree_mortality_files/figure-html/unnamed-chunk-24-1.png" width="768" /></p>
</div>
</div>
<div id="more-opportunities" class="section level2">
<h2>More opportunities</h2>
<p>There are a <em>bunch</em> of other variables in this dataset that would be worth considering - like how scorched the tree is post-fire, how large it was to start (height and diameter), evidence of beetle infestation, and more - I’m looking forward to coming back to this dataset in the future &amp; revisiting this model with additional investigation of those variables.</p>
</div>
